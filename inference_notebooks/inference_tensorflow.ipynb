{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5d3848-007c-40a3-84fc-e66740ab3257",
   "metadata": {},
   "source": [
    "# Overall\n",
    "\n",
    "How triton inference server is configured:\n",
    "\n",
    "1. Choose the Tensorflow model.\n",
    "2. Choose the backend or platform you want to deploy your model to.\n",
    "3. Set config and model checkpoints for the compiled model. The config will contain info about the backend/platform, input and output.\n",
    "4. Check if triton has loaded it or not.\n",
    "5. If loaded, define the input in tritonclient input wrapper and hit the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4fc703-cfa4-4157-8215-6b222ebc20c6",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef109e7-8b1f-4359-a8b2-c8f8bffb9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "model_name = 'joeddav/distilbert-base-uncased-go-emotions-student'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"tf\", max_length=256, padding='max_length')\n",
    "\n",
    "tick = time.time()\n",
    "logits, = model(**inputs)\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "predicted_label = model.config.id2label[tf.argmax(logits[0]).numpy()]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26cc5d6-c8ad-4d7f-bab4-9bf9a93da6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('../weights_tf')\n",
    "model.save_pretrained('../weights_tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115ea4b-f54b-4c61-b91e-b767c1838138",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Onnx backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb34b07-8c66-415a-b255-9296e0d8c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca2ead-21a4-4059-adb3-51e9aa348de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models_tf/onnx/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c55ab2-1afc-4dd7-aed0-64d89af619e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = ['INPUT0', 'INPUT1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c84e8-e8c4-4f84-a408-58b704e24b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tf2onnx\n",
    "import time\n",
    "import json\n",
    "from onnxruntime import InferenceSession\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "model_name = '../weights_tf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"tf\", max_length=256, padding='max_length')\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "\n",
    "# describe the inputs\n",
    "input_spec = (\n",
    "    tf.TensorSpec((None,  None), tf.int32, name=input_name[0]),\n",
    "    tf.TensorSpec((None,  None), tf.int32, name=input_name[1])\n",
    ")\n",
    "\n",
    "# and convert\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=input_spec, opset=13, output_path='../models_tf/onnx/1/model.onnx')\n",
    "output_name = [n.name for n in model_proto.graph.output]\n",
    "\n",
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']\n",
    "\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"np\", max_length=256, padding='max_length')\n",
    "session = InferenceSession(\"../models_tf/onnx/1/model.onnx\")\n",
    "\n",
    "tick = time.time()\n",
    "logits = session.run(output_names=output_name, input_feed={input_name[0]: inputs['input_ids'].astype(np.int32), input_name[1]: inputs['attention_mask'].astype(np.int32)})\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "id2label[str(logits[0][0].argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b18fb-492b-4da3-b7dc-8f2064388b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names = [n.name for n in model_proto.graph.output]\n",
    "output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d684d8-c662-4c1c-8008-523786cc72f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d701d53c-1ea6-45ca-9c32-82795e93800f",
   "metadata": {},
   "source": [
    "#### Send request to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc3968-03f4-44d6-ba41-9e4e73020984",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f626b9-9121-43dc-848b-0ebe8353178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbaa71-cca1-4565-bab1-3ca0ae0f93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba74db-f461-4576-a217-33ff73f356af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615be9c7-cc73-409b-a76f-5f04c1f2d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"onnx\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_tf/onnx/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce8afc-7dc2-460e-8903-52fd809b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['output_1']\n",
    "model_name = 'onnx'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacd4ee-b5c6-4f17-86a1-b6be58b13a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights_tf/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f574f71-de22-4b2e-ba7e-136bdd47be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights_tf/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913408e9-48a3-4177-855e-e4051ad9f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dc74e-350f-4b84-928e-8863401cb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits[0].argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31157b77-8ad9-40e2-9a1b-8ad23e7dc0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e108c0-a87e-48c0-8901-0def6bc1696b",
   "metadata": {},
   "source": [
    "### Triton backend (GPU only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63600eee-2b3f-4d86-b5b1-a044a548e532",
   "metadata": {},
   "source": [
    "#### Installation Guide\n",
    "\n",
    "Check the README."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25b56b-e9be-411e-9232-2f73ad8bd492",
   "metadata": {},
   "source": [
    "#### Create models\n",
    "\n",
    "Let's create 2 plans, one for fp32 and other one for fp16 (faster and uses less memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2fdf5-638a-404a-87b5-3ed15a8baee6",
   "metadata": {},
   "source": [
    "#### FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20371654-e0b3-4430-96a2-59c5246acd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models_tf/tensorrt_fp32/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847b220-14d0-4dbc-8d2d-bed85721f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --onnx=../models_tf/onnx/1/model.onnx --optShapes=INPUT0:16x256,INPUT1:16x256 --maxShapes=INPUT0:32x256,INPUT1:32x256 --minShapes=INPUT0:1x256,INPUT1:1x256 --shapes=INPUT0:1x256,INPUT1:1x256 --saveEngine=../models_tf/tensorrt_fp32/1/model.plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb40c45-71c3-4f66-947f-2dffc1dcf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd22cdd-1f3f-4c83-9a62-e3901be1c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/tensorrt_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f98ee-a3bb-4bef-837d-26d709b92cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"tensorrt_fp32_tf\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('../models_tf/tensorrt_fp32/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04965b32-ee68-4271-9190-38929f65a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213edfb-32d7-4233-b1a1-1a758b5be72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'tensorrt_fp32_tf'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa445404-5930-4bbd-8a31-2cd7e3b42fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae0bfe-c526-41d4-b021-04579cdb98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f1316-c916-46c5-b290-4e678ba6f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c34ce-3bd3-4828-b19c-c0f408dee5f9",
   "metadata": {},
   "source": [
    "#### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e327dee-e099-4068-aeda-630d435038c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models_tf/tensorrt_fp16/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6330955-428b-4de2-a7dd-677d824c0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --onnx=../models_tf/onnx/1/model.onnx --optShapes=INPUT0:16x256,INPUT1:16x256 --maxShapes=INPUT0:32x256,INPUT1:32x256 --minShapes=INPUT0:1x256,INPUT1:1x256 --shapes=INPUT0:1x256,INPUT1:1x256 --saveEngine=../models_tf/tensorrt_fp16/1/model.plan --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f131537-f196-4923-8a9d-1bee40621148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/tensorrt_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c8f32-8bd7-40bc-b1d0-13eaa997edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"tensorrt_fp16_tf\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_tf/tensorrt_fp16/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103e077-0393-492f-8dfe-70ddf74c10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'tensorrt_fp16_tf'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af247e-e089-4ea5-8ca1-789a90028e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f5e20-93df-47ab-9f40-fe27f09872f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b320a9a-da88-4fd0-91b5-1dd8b6e90e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c7795-56fe-4a9a-a90c-8eb3d4a24773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcee2c2-e451-4e59-ae1d-e935fd948cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232ee91-823f-4353-8ac5-a095d549f108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ee3c5-3f35-41dc-b3c0-1eb54864735f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0266f-9f67-4856-af83-9e2dc2b23996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
