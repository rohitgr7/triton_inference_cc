{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be13b45-db5e-4705-8c41-e9396036b466",
   "metadata": {},
   "source": [
    "### Dynamic batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269faa1-c835-4083-9beb-f583b243d919",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b21ceef-4774-4cd3-97d6-43c569536b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/python_dynamic_batching/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619134f2-c07f-4971-8e2f-79be970a71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['input_ids', 'attention_mask']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'python_dynamic_batching'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7412be-1bda-4de7-b77c-449c5076abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"python_dynamic_batching\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 32\n",
    "\n",
    "dynamic_batching { \n",
    "  preferred_batch_size: [ 4, 8, 16, 32 ] \n",
    "  max_queue_delay_microseconds: 3000000\n",
    "}\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models/python_dynamic_batching/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e9a364-e9c1-4344-9808-27081d51368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 0.0.0.0:8000...\n",
      "* Connected to 0.0.0.0 (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: 0.0.0.0:8000\n",
      "> User-Agent: curl/7.86.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host 0.0.0.0 left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c2032a-8c18-414f-bc71-d730b3792608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 0.0.0.0:8000...\n",
      "* Connected to 0.0.0.0 (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/python_dynamic_batching HTTP/1.1\n",
      "> Host: 0.0.0.0:8000\n",
      "> User-Agent: curl/7.86.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 266\n",
      "< \n",
      "* Connection #0 to host 0.0.0.0 left intact\n",
      "{\"name\":\"python_dynamic_batching\",\"versions\":[\"1\"],\"platform\":\"python\",\"inputs\":[{\"name\":\"input_ids\",\"datatype\":\"INT32\",\"shape\":[-1,256]},{\"name\":\"attention_mask\",\"datatype\":\"INT32\",\"shape\":[-1,256]}],\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[-1,28]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/python_dynamic_batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4bd8813-53db-4349-b930-4872a4736a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goku/miniconda3/envs/triton_cc/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-28 23:22:11.293166: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832b185a-c5aa-4b7b-b153-9d5d87a1955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['input_ids', 'attention_mask']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'python_dynamic_batching'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ba5d20-01d5-4921-b59c-db12e27ada65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fafc14-37aa-49ad-8c8e-f5366230b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0446c4b4-5fe4-47e0-b99e-a28b6222a554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.047541141510009766\n",
      "CPU times: user 745 ms, sys: 80.8 ms, total: 825 ms\n",
      "Wall time: 866 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "client = tritonhttpclient.InferenceServerClient(url=url, verbose=False, concurrency=32)\n",
    "# Encode the data using tokenizer\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "tick = time.time()\n",
    "\n",
    "# Define input config\n",
    "inputs = [\n",
    "    tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "    tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "]\n",
    "\n",
    "# Attach input\n",
    "inputs[0].set_data_from_numpy(input_ids)\n",
    "inputs[1].set_data_from_numpy(attention_mask)\n",
    "\n",
    "# Define output config\n",
    "outputs = [\n",
    "    tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "]\n",
    "\n",
    "# Hit triton server\n",
    "n_requests = 4\n",
    "responses = []\n",
    "\n",
    "for i in range(n_requests):\n",
    "    responses.append(client.async_infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs))\n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d79f11-7c1a-415e-bca3-9b5beaa9ba82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = responses[0].get_result().as_numpy(output_name[0])\n",
    "id2label[str(result[0].argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2486197-2239-4483-998c-81bad68f4480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf1bd5b7-bacf-48a1-885e-cba416c40ce4",
   "metadata": {},
   "source": [
    "#### Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a112f1eb-80de-4002-8ccf-cf092290599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/onnx_dynamic_batching/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3823483-e4b9-45d2-9607-0182682620bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['input_ids', 'attention_mask']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx_dynamic_batching'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0f229-9340-40fb-a306-a71656d2ecc7",
   "metadata": {},
   "source": [
    "1. `preferred_batch_size`: batches that the inference server should attempt to create.\n",
    "2. `max_queue_delay_microseconds`: If the `preferred_batch_size` can't be created, the server will delay until no request waits for more than `max_queue_delay_microseconds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbdb3986-8bf0-4561-ba79-c694958b57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"onnx_dynamic_batching\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "dynamic_batching { \n",
    "  preferred_batch_size: [ 4, 8, 16, 32 ] \n",
    "  max_queue_delay_microseconds: 3000000\n",
    "}\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models/onnx_dynamic_batching/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "072a8b93-87e9-4e04-80d2-384ec4927440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../models/onnx/1/ ../models/onnx_dynamic_batching/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f40758-f2c6-42b6-a8a1-83127f37d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 0.0.0.0:8000...\n",
      "* Connected to 0.0.0.0 (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: 0.0.0.0:8000\n",
      "> User-Agent: curl/7.86.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host 0.0.0.0 left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4f19360-7814-4f22-88f8-48bd8306dd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 0.0.0.0:8000...\n",
      "* Connected to 0.0.0.0 (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/onnx_dynamic_batching HTTP/1.1\n",
      "> Host: 0.0.0.0:8000\n",
      "> User-Agent: curl/7.86.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 274\n",
      "< \n",
      "* Connection #0 to host 0.0.0.0 left intact\n",
      "{\"name\":\"onnx_dynamic_batching\",\"versions\":[\"1\"],\"platform\":\"onnxruntime_onnx\",\"inputs\":[{\"name\":\"input_ids\",\"datatype\":\"INT32\",\"shape\":[-1,256]},{\"name\":\"attention_mask\",\"datatype\":\"INT32\",\"shape\":[-1,256]}],\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[-1,28]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/onnx_dynamic_batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa80d671-5b83-47fd-8660-071b5840150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d035aa1-d986-4768-ac4c-2156460f44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['input_ids', 'attention_mask']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx_dynamic_batching'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0308e377-5e9a-4fe3-992c-7a6abcb8bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a4b096-7d86-4b7f-83cc-77f47bd1e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74eb23e-b23c-4c2f-9240-2870b65eb1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.04227113723754883\n",
      "CPU times: user 5.95 ms, sys: 2.86 ms, total: 8.81 ms\n",
      "Wall time: 488 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False, concurrency=32) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    n_requests = 4\n",
    "    responses = []\n",
    "    \n",
    "    for i in range(n_requests):\n",
    "        responses.append(client.async_infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs))\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7d7f0b2-07e1-49b4-b986-c1f31f2109ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = responses[0].get_result().as_numpy(output_name[0])\n",
    "id2label[str(result[0].argmax())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dd2a4-11d2-4d2f-a8fa-b178f077d681",
   "metadata": {},
   "source": [
    "#### With GRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46d2a05b-14fa-447f-ac80-698ae7a3ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as tritongrpcclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7f3707e-bb7e-49c6-8b86-8f0e3d6d1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['input_ids', 'attention_mask']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx_dynamic_batching'\n",
    "url = '0.0.0.0:8001'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22b4ff7a-572b-43b7-b651-315b14822ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a755c6f-5547-4d20-971c-29ef7321d864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0011510848999023438\n",
      "CPU times: user 4.33 ms, sys: 2.1 ms, total: 6.43 ms\n",
      "Wall time: 5.53 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "client = tritongrpcclient.InferenceServerClient(url=url, verbose=False)\n",
    "results = []\n",
    "\n",
    "def callback(user_data, result, error):\n",
    "    if error:\n",
    "        user_data.append(error)\n",
    "    else:\n",
    "        user_data.append(result)\n",
    "\n",
    "# Encode the data using tokenizer\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "tick = time.time()\n",
    "\n",
    "# Define input config\n",
    "inputs = [\n",
    "    tritongrpcclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "    tritongrpcclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "]\n",
    "\n",
    "# Attach input\n",
    "inputs[0].set_data_from_numpy(input_ids)\n",
    "inputs[1].set_data_from_numpy(attention_mask)\n",
    "\n",
    "# Define output config\n",
    "outputs = [\n",
    "    tritongrpcclient.InferRequestedOutput(output_name[0]),\n",
    "]\n",
    "\n",
    "# Hit triton server\n",
    "n_requests = 4\n",
    "responses = []\n",
    "\n",
    "for i in range(n_requests):\n",
    "    responses.append(client.async_infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs, callback=partial(callback, results)))\n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4b4eca4-7f09-4f34-8262-6c5c6fb2d01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = results[0].as_numpy(output_name[0])\n",
    "id2label[str(result[0].argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4e74e-aed8-4a5a-90f4-60787a596d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb197049-585d-489d-96e3-f0e1926a0faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e10bf-9fd6-4bc4-8d58-83d1003ca198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
