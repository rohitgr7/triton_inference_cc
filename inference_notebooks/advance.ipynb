{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be13b45-db5e-4705-8c41-e9396036b466",
   "metadata": {},
   "source": [
    "### Dynamic batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112f1eb-80de-4002-8ccf-cf092290599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/onnx_dynamic_batching/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3823483-e4b9-45d2-9607-0182682620bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx_dynamic_batching'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0f229-9340-40fb-a306-a71656d2ecc7",
   "metadata": {},
   "source": [
    "1. `preferred_batch_size`: batches that the inference server should attempt to create.\n",
    "2. `max_queue_delay_microseconds`: If the `preferred_batch_size` can't be created, the server will delay until no request waits for more than `max_queue_delay_microseconds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb3986-8bf0-4561-ba79-c694958b57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"onnx_dynamic_batching\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "dynamic_batching { \n",
    "  preferred_batch_size: [ 4, 8, 16, 32 ] \n",
    "  max_queue_delay_microseconds: 3000000\n",
    "}\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models/onnx_dynamic_batching/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a8b93-87e9-4e04-80d2-384ec4927440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../models/onnx/1/ ../models/onnx_dynamic_batching/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f40758-f2c6-42b6-a8a1-83127f37d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f19360-7814-4f22-88f8-48bd8306dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/onnx_dynamic_batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80d671-5b83-47fd-8660-071b5840150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d035aa1-d986-4768-ac4c-2156460f44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx_dynamic_batching'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308e377-5e9a-4fe3-992c-7a6abcb8bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4b096-7d86-4b7f-83cc-77f47bd1e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74eb23e-b23c-4c2f-9240-2870b65eb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False, concurrency=32) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    n_requests = 4\n",
    "    responses = []\n",
    "    \n",
    "    for i in range(n_requests):\n",
    "        responses.append(client.async_infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs))\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7f0b2-07e1-49b4-b986-c1f31f2109ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = responses[0].get_result().as_numpy(output_name[0])\n",
    "id2label[str(result[0].argmax())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dd2a4-11d2-4d2f-a8fa-b178f077d681",
   "metadata": {},
   "source": [
    "#### With GRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2a05b-14fa-447f-ac80-698ae7a3ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as tritongrpcclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3707e-bb7e-49c6-8b86-8f0e3d6d1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx_dynamic_batching'\n",
    "url = '0.0.0.0:8001'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4ff7a-572b-43b7-b651-315b14822ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a755c6f-5547-4d20-971c-29ef7321d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "client = tritongrpcclient.InferenceServerClient(url=url, verbose=False)\n",
    "results = []\n",
    "\n",
    "def callback(user_data, result, error):\n",
    "    if error:\n",
    "        user_data.append(error)\n",
    "    else:\n",
    "        user_data.append(result)\n",
    "\n",
    "# Encode the data using tokenizer\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "tick = time.time()\n",
    "\n",
    "# Define input config\n",
    "inputs = [\n",
    "    tritongrpcclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "    tritongrpcclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "]\n",
    "\n",
    "# Attach input\n",
    "inputs[0].set_data_from_numpy(input_ids)\n",
    "inputs[1].set_data_from_numpy(attention_mask)\n",
    "\n",
    "# Define output config\n",
    "outputs = [\n",
    "    tritongrpcclient.InferRequestedOutput(output_name[0]),\n",
    "]\n",
    "\n",
    "# Hit triton server\n",
    "n_requests = 4\n",
    "responses = []\n",
    "\n",
    "for i in range(n_requests):\n",
    "    responses.append(client.async_infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs, callback=partial(callback, results)))\n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4eca4-7f09-4f34-8262-6c5c6fb2d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results[0].as_numpy(output_name[0])\n",
    "id2label[str(result[0].argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ca84c-bc71-4c5f-bad9-dc7524f46d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4e74e-aed8-4a5a-90f4-60787a596d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb197049-585d-489d-96e3-f0e1926a0faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e10bf-9fd6-4bc4-8d58-83d1003ca198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
