{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcf925f-1c04-4a90-9c57-86332f7bec31",
   "metadata": {},
   "source": [
    "Inspired by: https://github.com/triton-inference-server/server/tree/main/docs/examples/stable_diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482086b7-4eb6-4bdf-afc9-06324ce8e1bb",
   "metadata": {},
   "source": [
    "- When it comes to multi-model pipeline where we don't have a single model by a bunch of models within a single pipeline, we could pull out those models (nn.Module) and deploy each of them onto different backends (onnx, torchscript, TensorRT) etc and keep the pipeline in the python runtime.\n",
    "\n",
    "Let's try to compile all of these models in all backends. Let's do it step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337b1eb-8942-40b5-ade7-70df75cf84a3",
   "metadata": {},
   "source": [
    "- To build individual models, you need to understand what kind of models are there in the pipeline.\n",
    "One by one load them individually and compile them as per your required backend.\n",
    "- For diffusion pipeline, it has `UNet2DConditionModel`(Unet), `CLIPTextModel`(Text Encoder), `AutoencoderKL`(VAE)\n",
    "\n",
    "- Additional components are `CLIPTokenizer`(Tokenizer) and `DPMSolverMultistepScheduler`(Scheduler). These components can't be compiled so they will stay in Python runtime.\n",
    "\n",
    "- Check pre-trained weights here: https://huggingface.co/stabilityai/stable-diffusion-2-base/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d90f73-e6e9-4f3c-bf08-7331dfe46843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from diffusers import DPMSolverMultistepScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98434358-f382-470f-afe3-1e6ad2a0c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fb739-6b89-4575-95cd-bb4e4754018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DPMSolverMultistepScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90f7de-049b-4166-b69a-1e4f14dc0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('../weights_sd/tokenizer')\n",
    "scheduler.save_pretrained('../weights_sd/scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a3ff77-f6dc-4891-bdc0-10c3d51fbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTextModel\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46948f14-91ac-4bf1-ae4f-664b02b5f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_TYPE = \"GPU\" if torch.cuda.is_available() else \"CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f9042-423a-40c4-a7f3-21960fec7cdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ee6a3-ed5d-4f63-8d6c-eb9b7986b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models_sd/onnx/text_encoder/1\n",
    "!mkdir -p ../models_sd/onnx/vae/1\n",
    "!mkdir -p ../models_sd/onnx/unet/1\n",
    "!mkdir -p ../models_sd/onnx/pipeline/1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71610489-7799-4fa7-a80c-5fc5d2de7eb0",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b45c0-4c8e-4b76-bdda-d4331da0aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CLIPTextModel.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"text_encoder\", return_dict=False)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        return self.model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe425b9-df34-41f9-9ce5-201b978b1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoderModel()\n",
    "text_encoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab1362-462a-475a-9ee5-607a7c06ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'real life goku going super saiyan, beautiful landscape, lightning storm, dramatic lightning, cinematic, establishing shot'\n",
    "text_input = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae73e7-8c1c-42ba-894b-5271fba4113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = ['input_ids']\n",
    "OUTPUT_NAMES = ['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88648493-e129-4d5b-9424-36260e9d899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = f\"\"\"\n",
    "name: \"text_encoder\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"{OUTPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 1024 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    kind: KIND_{DEVICE_TYPE}\n",
    "  }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_sd/onnx/text_encoder/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191a17f-f53d-4560-bbd9-e5fcd4b750e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        text_encoder,\n",
    "        text_input.input_ids.to(torch.int32),\n",
    "        \"../models_sd/onnx/text_encoder/1/model.onnx\",\n",
    "        input_names=INPUT_NAMES,\n",
    "        output_names=OUTPUT_NAMES,\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {\n",
    "                0: \"batch_size\",\n",
    "                1: \"sequence_len\"\n",
    "            },\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7197ae-0f5e-43ae-b160-6795afe53549",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9444d16-ccba-440b-9894-5fed7d453988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = UNet2DConditionModel.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"unet\", return_dict=False)\n",
    "        \n",
    "    def forward(self, latent_model_input, t, prompt_embeds):\n",
    "        return self.model(latent_model_input, t, encoder_hidden_states=prompt_embeds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77efe20-4028-4870-84e1-831d84ce0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UnetModel()\n",
    "unet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a551dd-13d3-4fca-9e69-79cac1cf8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = ['latents', 'timestep', 'prompt_embeds']\n",
    "OUTPUT_NAMES = ['latents_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4dcd7-f405-4f01-8d74-77ac82324f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = f\"\"\"\n",
    "name: \"unet\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "default_model_filename: \"model.onnx\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[1]}\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "    reshape: {{ shape: [ ] }}\n",
    "  }}\n",
    "]\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[2]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 1024 ]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"{OUTPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 4, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    kind: KIND_{DEVICE_TYPE}\n",
    "  }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_sd/onnx/unet/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c97032-f644-4fa1-9079-c65415217f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import OnnxRuntimeModel\n",
    "unet=OnnxRuntimeModel.from_pretrained('../models_sd/onnx/unet/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9243ed1-b041-4b69-89d3-0691331d28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../models_sd/onnx/unet/1/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb4171-21a6-4180-99dd-2ecc619f612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        unet,\n",
    "        (torch.randn(2, 4, 64, 64),  torch.tensor([7, 7]).int(), torch.randn(2, 77, 1024)),\n",
    "        \"../models_sd/onnx/unet/1/model.onnx\",\n",
    "        input_names=INPUT_NAMES,\n",
    "        output_names=OUTPUT_NAMES,\n",
    "        dynamic_axes={\n",
    "            INPUT_NAMES[0]: {\n",
    "                0: \"batch_size\",\n",
    "                1: \"channels\",\n",
    "                2: \"height\",\n",
    "                3: \"width\",\n",
    "            },\n",
    "            INPUT_NAMES[1]: {\n",
    "                0: \"batch_size\",\n",
    "            },\n",
    "            INPUT_NAMES[2]: {\n",
    "                0: \"batch_size\",\n",
    "                1: \"sequence_len\",\n",
    "            },\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1974a43-4ebf-4755-a59a-563947dcbb00",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78c7e6-e5bd-47a3-a7b2-6dcf7d2018da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoencoderKL.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"vae\", return_dict=False)    \n",
    "\n",
    "    def forward(self, latents):\n",
    "        return self.model.decode(latents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05801223-ef4e-4525-96cc-02a349c4e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAEModel()\n",
    "vae.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa316fe7-34da-4829-ad13-35cb61e2a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = ['latents']\n",
    "OUTPUT_NAMES = ['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916017a-4906-4687-89eb-feea653f887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = f\"\"\"\n",
    "name: \"vae\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"{OUTPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    kind: KIND_{DEVICE_TYPE}\n",
    "  }}\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_sd/onnx/vae/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e3006-4952-4073-85c9-7c0c2af97b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        vae,\n",
    "        torch.randn(1, 4, 64, 64),\n",
    "        \"../models_sd/onnx/vae/1/model.onnx\",\n",
    "        input_names=INPUT_NAMES,\n",
    "        output_names=OUTPUT_NAMES,\n",
    "        dynamic_axes={\n",
    "            INPUT_NAMES[0]: {\n",
    "                0: \"batch_size\",\n",
    "                1: \"channels\",\n",
    "                2: \"height\",\n",
    "                3: \"width\",\n",
    "            },\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888bab3e-34d8-4e8e-aeed-f3d11584cb28",
   "metadata": {},
   "source": [
    "## Torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fec30a-82d8-48ed-8768-bada033a2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models_sd/torchscript/text_encoder/1\n",
    "!mkdir -p ../models_sd/torchscript/vae/1\n",
    "!mkdir -p ../models_sd/torchscript/unet/1\n",
    "!mkdir -p ../models_sd/torchscript/pipeline/1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725cd88e-38be-4594-89b3-dd178359c7f8",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ab311-3ba5-46a6-a90b-580157926f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CLIPTextModel.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"text_encoder\", return_dict=False)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        return self.model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa825b9-c65a-46a9-a70f-3c8159be74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoderModel()\n",
    "text_encoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20df777-a67a-4676-bcea-ce8152179858",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'real life goku going super saiyan, beautiful landscape, lightning storm, dramatic lightning, cinematic, establishing shot'\n",
    "text_input = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2e7da-a86f-4154-80ee-2f222e81c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = ['input_ids']\n",
    "OUTPUT_NAMES = ['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff780d8-db44-4370-bb00-0a0a84ce69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = f\"\"\"\n",
    "name: \"text_encoder\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"{OUTPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 1024 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    kind: KIND_{DEVICE_TYPE}\n",
    "  }}\n",
    "]\n",
    "\n",
    "parameters: {{\n",
    "    key: \"INFERENCE_MODE\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_sd/torchscript/text_encoder/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed4099-801a-46f8-ab92-a0ace00db07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    traced_script_module = torch.jit.trace(text_encoder, (text_input.input_ids.to(torch.int32),))\n",
    "    traced_script_module.save('../models_sd/torchscript/text_encoder/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4663b-db84-46e1-8df0-a6a4248c0145",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6054e4-6b23-4c60-9d12-7f37b593c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = UNet2DConditionModel.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"unet\", return_dict=False)\n",
    "        \n",
    "    def forward(self, latent_model_input, t, prompt_embeds):\n",
    "        return self.model(latent_model_input, t, encoder_hidden_states=prompt_embeds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb234db1-5616-4366-86b7-fe02ffdfc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UnetModel()\n",
    "unet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a252e-8686-42ab-a052-d06fdabe39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = ['latents', 'timestep', 'prompt_embeds']\n",
    "OUTPUT_NAMES = ['latents_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f45eb7-339d-4b30-8050-6556831b4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = f\"\"\"\n",
    "name: \"unet\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[1]}\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "    reshape: {{ shape: [ ] }}\n",
    "  }}\n",
    "]\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[2]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 1024 ]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"{OUTPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 4, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    kind: KIND_{DEVICE_TYPE}\n",
    "  }}\n",
    "]\n",
    "\n",
    "parameters: {{\n",
    "    key: \"INFERENCE_MODE\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_sd/torchscript/unet/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814a3e4-cd9d-4bae-b96c-2e09426bc893",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    traced_script_module = torch.jit.trace(unet, (torch.randn(2, 4, 64, 64), torch.tensor([7, 7]).int(), torch.randn(2, 77, 1024)))\n",
    "    traced_script_module.save('../models_sd/torchscript/unet/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a988f-98b8-4f84-b374-d61040ede134",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9ac1f-99c7-41d2-9d3c-ca9b4b0c2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoencoderKL.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=\"vae\", return_dict=False)    \n",
    "\n",
    "    def forward(self, latents):\n",
    "        return self.model.decode(latents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de44c6-3c10-4196-beb4-0704a7ea8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAEModel()\n",
    "vae.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322b7b9-3a85-47d7-9d55-aa4440abb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = ['latents']\n",
    "OUTPUT_NAMES = ['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2735a24-6a42-4404-a4eb-5c2bc57046ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = f\"\"\"\n",
    "name: \"vae\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"{OUTPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3, -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    kind: KIND_{DEVICE_TYPE}\n",
    "  }}\n",
    "]\n",
    "\n",
    "parameters: {{\n",
    "    key: \"INFERENCE_MODE\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_sd/torchscript/vae/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c35178-b7af-4342-ad3f-7beb01fe58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    traced_script_module = torch.jit.trace(vae, (torch.randn(1, 4, 64, 64),))\n",
    "    traced_script_module.save('../models_sd/torchscript/vae/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504391fe-88bf-47a4-baaf-12847310aa9e",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c697674-f4d2-4a31-957c-5e02aae837ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = ['prompt', 'height', 'width', 'inference_steps']\n",
    "OUTPUT_NAMES = ['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76081e9-7503-40b6-9b4e-ded2569904c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = f\"\"\"\n",
    "name: \"pipeline\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 0\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[1]}\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[2]}\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "input [\n",
    "  {{\n",
    "    name: \"{INPUT_NAMES[3]}\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"{OUTPUT_NAMES[0]}\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1, -1]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    kind: KIND_{DEVICE_TYPE}\n",
    "  }}\n",
    "]\n",
    "\n",
    "parameters {{\n",
    "    key: \"unet_inchannels\"\n",
    "    value: {{ string_value: \"{unet.model.in_channels}\" }}\n",
    "}}\n",
    "\n",
    "parameters {{\n",
    "    key: \"vae_scale_factor\"\n",
    "    value: {{ string_value: \"{2**(len(vae.model.config.block_out_channels) - 1)}\" }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models_sd/torchscript/pipeline/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920055e6-2169-4f13-9e41-af062640bfe4",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3497f4-4796-48f7-8e74-a936a2f6e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605380f-c358-4a3b-a04f-2991fe7472a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ee5c9-1207-43eb-bfc0-48eb53a0792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ba28c-b6ac-41f0-a24b-411a6f73dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "INPUT_NAMES = ['prompt', 'height', 'width', 'inference_steps']\n",
    "OUTPUT_NAMES = ['image']\n",
    "INPUT_DTYPES = ['BYTES', 'INT32', 'INT32', 'INT32']\n",
    "OUTPUT_DTYPES = ['FLOAT32']\n",
    "model_name = 'pipeline'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146aa776-ceb6-4707-acd4-8c47178e4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['real life goku going super saiyan, beautiful landscape, lightning storm, dramatic lightning, cinematic, establishing shot']*2\n",
    "# text_input = tokenizer(\n",
    "#     prompt,\n",
    "#     padding=\"max_length\",\n",
    "#     max_length=tokenizer.model_max_length,\n",
    "#     truncation=True,\n",
    "#     return_tensors=\"pt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5fc57-4137-429d-ac7b-76d4660709c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(INPUT_NAMES[0], (len(prompts),), INPUT_DTYPES[0]),\n",
    "        tritonhttpclient.InferInput(INPUT_NAMES[1], (1,), INPUT_DTYPES[1]),\n",
    "        tritonhttpclient.InferInput(INPUT_NAMES[2], (1,), INPUT_DTYPES[2]),\n",
    "        tritonhttpclient.InferInput(INPUT_NAMES[3], (1,), INPUT_DTYPES[3]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(np.asarray(prompts, dtype=object))\n",
    "    inputs[1].set_data_from_numpy(np.asarray([512], dtype=np.int32))\n",
    "    inputs[2].set_data_from_numpy(np.asarray([512], dtype=np.int32))\n",
    "    inputs[3].set_data_from_numpy(np.asarray([2], dtype=np.int32))\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(OUTPUT_NAMES[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    generated_images = response.as_numpy(OUTPUT_NAMES[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b1df4-2c97-43e5-989b-1d5396e62816",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = (generated_images*255).round().astype(\"uint8\")\n",
    "generated_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616045f1-c9f5-4f0a-82b8-740a788dacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(generated_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411e3a6-d188-4f7a-bd37-c3f8fcd24833",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(generated_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3583c-0620-4b60-85fa-90764b269516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda02995-5cdb-4c77-b01c-87226bda6b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d84df2-f31f-44b8-8f48-2ab163ba86a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d43cb-2710-4c44-af93-56e2a902f964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
