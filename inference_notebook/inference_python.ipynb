{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall\n",
    "\n",
    "How triton inference server is configured:\n",
    "\n",
    "1. Choose the PyTorch model.\n",
    "2. Choose the backend or platform you want to deploy your model to.\n",
    "3. Set config and model checkpoints for the compiled model. The config will contain info about the backend/platform, input and output.\n",
    "4. Check if triton has loaded it or not.\n",
    "5. If loaded, define the input in tritonclient input wrapper and hit the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "model_name = 'joeddav/distilbert-base-uncased-go-emotions-student'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "\n",
    "tick = time.time()\n",
    "with torch.inference_mode():\n",
    "    logits = model(**inputs)\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "predicted_label = model.config.id2label[logits[0].argmax().item()]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('../weights')\n",
    "model.save_pretrained('../weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/triton-inference-server/python_backend/blob/main/examples/pytorch/model.py\n",
    "2. https://github.com/triton-inference-server/python_backend/blob/main/examples/pytorch/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/pytorch/1\n",
    "!touch ../models/pytorch/1/model.py\n",
    "\n",
    "## And create those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"pytorch\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 32\n",
    "\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "  }\n",
    "\n",
    "instance_group [\n",
    " {\n",
    "    count: 1\n",
    "    kind: KIND_CPU\n",
    " }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models/pytorch/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send request to server\n",
    "\n",
    "We will be using tritonclient to hit the API. Check [installation instructions](https://github.com/triton-inference-server/client#download-using-python-package-installer-pip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'pytorch'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Onnx backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/onnx/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"onnx\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models/onnx/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import json\n",
    "from onnxruntime import InferenceSession\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "class OnnxModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)[0]  # logits\n",
    "\n",
    "\n",
    "model_name = '../weights'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "\n",
    "model = OnnxModel(model_name)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    torch.onnx.export(model, (inputs['input_ids'].type(torch.int32), inputs['attention_mask'].type(torch.int32)), \n",
    "                      '../models/onnx/1/model.onnx', verbose=False, \n",
    "                      input_names=input_name, output_names=output_name, \n",
    "                      dynamic_axes={input_name[0]: {0: 'batch_size'}, input_name[1]: {0: 'batch_size'}, output_name[0]: {0: 'batch_size'}})\n",
    "\n",
    "\n",
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']\n",
    "\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"np\", max_length=256, padding='max_length')\n",
    "session = InferenceSession(\"../models/onnx/1/model.onnx\")\n",
    "\n",
    "tick = time.time()\n",
    "with torch.inference_mode():\n",
    "    logits = session.run(output_names=output_name, input_feed={input_name[0]: inputs['input_ids'].astype(np.int32), input_name[1]: inputs['attention_mask'].astype(np.int32)})\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "id2label[str(logits[0][0].argmax().item())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send request to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits[0].argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TorchScript backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/torchscript/1\n",
    "\n",
    "## And create those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"torchscript\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "  }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models/torchscript/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "class TorchScriptModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)[0]  # logits\n",
    "\n",
    "\n",
    "model_name = '../weights'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "\n",
    "model = TorchScriptModel(model_name)\n",
    "model.eval()\n",
    "traced_script_module = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask']))\n",
    "traced_script_module.save('../models/torchscript/1/model.pt')\n",
    "\n",
    "tick = time.time()\n",
    "with torch.inference_mode():\n",
    "    logits = traced_script_module(inputs['input_ids'], inputs['attention_mask'])\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']\n",
    "\n",
    "    \n",
    "id2label[str(logits.argmax().item())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send request to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'torchscript'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triton backend (GPU only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation Guide\n",
    "\n",
    "Check the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create models\n",
    "\n",
    "Let's create 2 plans, one for fp32 and other one for fp16 (faster and uses less memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/tensorrt_fp32/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --onnx=../models/onnx/1/model.onnx --optShapes=INPUT0:16x256,INPUT1:16x256 --maxShapes=INPUT0:32x256,INPUT1:32x256 --minShapes=INPUT0:1x256,INPUT1:1x256 --shapes=INPUT0:1x256,INPUT1:1x256 --saveEngine=../models/tensorrt_fp32/1/model.plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/tensorrt_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"tensorrt_fp32\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('../models/tensorrt_fp32/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'tensorrt_fp32'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/tensorrt_fp16/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --onnx=../models/onnx/1/model.onnx --optShapes=INPUT0:16x256,INPUT1:16x256 --maxShapes=INPUT0:32x256,INPUT1:32x256 --minShapes=INPUT0:1x256,INPUT1:1x256 --shapes=INPUT0:1x256,INPUT1:1x256 --saveEngine=../models/tensorrt_fp16/1/model.plan --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/tensorrt_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"tensorrt_fp16\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  } ,\n",
    "{\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 28 ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('../models/tensorrt_fp16/config.pbtxt', 'w') as f:\n",
    "    f.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'tensorrt_fp16'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
