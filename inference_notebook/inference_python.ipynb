{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall\n",
    "\n",
    "How triton inference server is configured:\n",
    "\n",
    "1. Choose the PyTorch model.\n",
    "2. Choose the backend or platform you want to deploy your model to.\n",
    "3. Set config and model checkpoints for the compiled model. The config will contain info about the backend/platform, input and output.\n",
    "4. Check if triton has loaded it or not.\n",
    "5. If loaded, define the input in tritonclient input wrapper and hit the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goku/miniconda3/envs/triton_cc/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.12365293502807617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "model_name = 'joeddav/distilbert-base-uncased-go-emotions-student'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "\n",
    "tick = time.time()\n",
    "with torch.inference_mode():\n",
    "    logits = model(**inputs)\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "predicted_label = model.config.id2label[logits[0].argmax().item()]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('../weights')\n",
    "model.save_pretrained('../weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/triton-inference-server/python_backend/blob/main/examples/pytorch/model.py\n",
    "2. https://github.com/triton-inference-server/python_backend/blob/main/examples/pytorch/config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/pytorch/1\n",
    "!touch ../models/pytorch/config.pbtxt\n",
    "!touch ../models/pytorch/1/model.py\n",
    "\n",
    "## And create those files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send request to server\n",
    "\n",
    "We will be using tritonclient to hit the API. Check [installation instructions](https://github.com/triton-inference-server/client#download-using-python-package-installer-pip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goku/miniconda3/envs/triton_cc/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'pytorch'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.18004775047302246\n"
     ]
    }
   ],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Onnx backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ../models/onnx/1\n",
    "# !touch ../models/onnx/config.pbtxt\n",
    "\n",
    "# And create those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goku/miniconda3/envs/triton_cc/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:217: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.13427281379699707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import json\n",
    "from onnxruntime import InferenceSession\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "class OnnxModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)[0]  # logits\n",
    "\n",
    "\n",
    "model_name = '../weights'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "\n",
    "model = OnnxModel(model_name)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    torch.onnx.export(model, (inputs['input_ids'].type(torch.int32), inputs['attention_mask'].type(torch.int32)), \n",
    "                      '../models/onnx/1/model.onnx', verbose=False, \n",
    "                      input_names=input_name, output_names=output_name, \n",
    "                      dynamic_axes={input_name[0]: {0: 'batch_size'}, input_name[1]: {0: 'batch_size'}, output_name[0]: {0: 'batch_size'}})\n",
    "\n",
    "\n",
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']\n",
    "\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"np\", max_length=256, padding='max_length')\n",
    "session = InferenceSession(\"../models/onnx/1/model.onnx\")\n",
    "\n",
    "tick = time.time()\n",
    "with torch.inference_mode():\n",
    "    logits = session.run(output_names=output_name, input_feed={input_name[0]: inputs['input_ids'].astype(np.int32), input_name[1]: inputs['attention_mask'].astype(np.int32)})\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "id2label[str(logits[0][0].argmax().item())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send request to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'onnx'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.12295889854431152\n"
     ]
    }
   ],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(logits[0].argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TorchScript backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/torchscript/1\n",
    "!touch ../models/torchscript/config.pbtxt\n",
    "\n",
    "## And create those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.16965603828430176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "class TorchScriptModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=False)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)[0]  # logits\n",
    "\n",
    "\n",
    "model_name = '../weights'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I feel lucky to be here.\", return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "\n",
    "model = TorchScriptModel(model_name)\n",
    "model.eval()\n",
    "traced_script_module = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask']))\n",
    "traced_script_module.save('../models/torchscript/1/model.pt')\n",
    "\n",
    "tick = time.time()\n",
    "with torch.inference_mode():\n",
    "    logits = traced_script_module(inputs['input_ids'], inputs['attention_mask'])\n",
    "    \n",
    "tock = time.time()\n",
    "print(f'Time taken: {tock - tick}')\n",
    "\n",
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']\n",
    "\n",
    "    \n",
    "id2label[str(logits.argmax().item())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send request to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's restart the notebook\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 0.0.0.0:8000...\n",
      "* Connected to 0.0.0.0 (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: 0.0.0.0:8000\n",
      "> User-Agent: curl/7.84.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host 0.0.0.0 left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 0.0.0.0:8000...\n",
      "* Connected to 0.0.0.0 (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/models/torchscript HTTP/1.1\n",
      "> Host: 0.0.0.0:8000\n",
      "> User-Agent: curl/7.84.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 253\n",
      "< \n",
      "* Connection #0 to host 0.0.0.0 left intact\n",
      "{\"name\":\"torchscript\",\"versions\":[\"1\"],\"platform\":\"pytorch_libtorch\",\"inputs\":[{\"name\":\"INPUT0\",\"datatype\":\"INT32\",\"shape\":[-1,256]},{\"name\":\"INPUT1\",\"datatype\":\"INT32\",\"shape\":[-1,256]}],\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"FP32\",\"shape\":[-1,28]}]}"
     ]
    }
   ],
   "source": [
    "!curl -v 0.0.0.0:8000/v2/models/torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../weights/config.json', 'r') as f:\n",
    "    id2label = json.load(f)['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = ['INPUT0', 'INPUT1']\n",
    "input_dtype = ['INT32', 'INT32']\n",
    "output_name = ['OUTPUT0']\n",
    "model_name = 'torchscript'\n",
    "url = '0.0.0.0:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../weights/')\n",
    "text = 'I feel lucky to be here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.12454485893249512\n"
     ]
    }
   ],
   "source": [
    "with tritonhttpclient.InferenceServerClient(url=url, verbose=False) as client:\n",
    "    # Encode the data using tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, padding='max_length')\n",
    "    input_ids = np.array(inputs['input_ids'], dtype=np.int32)\n",
    "    attention_mask = np.array(inputs['attention_mask'], dtype=np.int32)\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Define input config\n",
    "    inputs = [\n",
    "        tritonhttpclient.InferInput(input_name[0], input_ids.shape, input_dtype[0]),\n",
    "        tritonhttpclient.InferInput(input_name[1], attention_mask.shape, input_dtype[1]),\n",
    "    ]\n",
    "    \n",
    "    # Attach input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(attention_mask)\n",
    "    \n",
    "    # Define output config\n",
    "    outputs = [\n",
    "        tritonhttpclient.InferRequestedOutput(output_name[0]),\n",
    "    ]\n",
    "    \n",
    "    # Hit triton server\n",
    "    response = client.infer(model_name, model_version=model_version, inputs=inputs, outputs=outputs)\n",
    "    logits = response.as_numpy(output_name[0])\n",
    "    tock = time.time()\n",
    "    print(f'Time taken: {tock - tick}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relief'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(logits.argmax())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
